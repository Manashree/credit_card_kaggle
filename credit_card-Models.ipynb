{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mani/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature description\n",
    "'PCA' features - V1, V2, ... V28 <br>\n",
    "'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. <br>\n",
    "'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. <br>\n",
    "'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. <br>\n",
    "\n",
    "492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the class distributions\n",
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating time based features:\n",
    "\n",
    "# separate transactions by day - 1 for day2 or 0 for day1\n",
    "#df['day'] = df['Time'].map( lambda x: 1 if x > 3600 * 24 else 0)\n",
    "\n",
    "## make transaction relative to day\n",
    "#df['time_of_day'] = df.apply(lambda row: (row['Time']-86400) if(row['day']==1) else(row['Time']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle the data so that it becomes truly random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will create train-test splits here itself and then work with resampling techniques as in real world the assumption is that test data is always unseen,and hence cannot be resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  199364\n",
      "Number transactions test dataset:  85443\n",
      "Total number of transactions:  284807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y,test_size = 0.30, random_state = 999)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique 1 - SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "sm1 = SMOTEENN()\n",
    "X_SMOTEENN, Y_SMOTEENN = sm1.fit_sample(X_train, Y_train)\n",
    "df_SMOTEENN = pd.DataFrame(X_SMOTEENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique 2 - SMOTETOMEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "sm2 = SMOTETomek()\n",
    "\n",
    "X_SMOTETomek, Y_SMOTETomek = sm2.fit_sample(X_train, Y_train)\n",
    "df_SMOTETomek = pd.DataFrame(X_SMOTETomek)\n",
    "#df_SMOTETomek = df_SMOTETomek.assign(Class=Y_SMOTETomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = df_SMOTEENN\n",
    "input_labels = Y_SMOTEENN\n",
    "test_data = X_test\n",
    "test_labels = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_data = shuffle(df_SMOTEENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With resampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    'lr' : LogisticRegression(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    'gbc': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "model_parameters = {\n",
    "        'rfc':{\n",
    "            'n_estimators': [50 ,100, 30], \n",
    "            'max_depth': [None, 1, 2, 3, 5], \n",
    "            'min_samples_split': [2, 3, 5]\n",
    "        },\n",
    "        'gbc':{\n",
    "            'loss':['deviance', 'exponential'],\n",
    "            'n_estimators': [50, 100, 150], \n",
    "            'criterion':['friedman_mse','mae'],\n",
    "            'max_depth': [None, 1, 2, 3, 5], \n",
    "            'min_samples_split': [2, 3, 5]\n",
    "        },\n",
    "        'lr':{\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'penalty':['l1','l2']\n",
    "        }\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for m in model:\n",
    "    print(\"Executing model - \",m)\n",
    "    clf = GridSearchCV(estimator = model[m], param_grid = model_parameters[m], cv=skf, scoring='recall')\n",
    "    clf.fit(input_data, input_labels)\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    test_pred = clf.predict(test_data)\n",
    "\n",
    "    #print(\"Grid scores: \", clf.grid_scores_)\n",
    "    #print(\"Best score on training set: \", clf.best_score_)\n",
    "    print('Accuracy: ', str(np.round(accuracy_score(test_labels, test_pred),5)))\n",
    "    print('Cohen Kappa: ' + str(np.round(cohen_kappa_score(test_labels, test_pred),5)))\n",
    "    print('Recall: ' + str(np.round(recall_score(test_labels, test_pred),5)))\n",
    "    #print('F1: ' + str(np.round(f1_score(test_labels, test_pred),5)))\n",
    "    \n",
    "    #model dump\n",
    "    \n",
    "    print('Writing best estimator to file for: ', m)\n",
    "    fname = m + '-model.pkl'\n",
    "    with open(fname,'wb') as f:\n",
    "        pickle.dump(clf.best_estimator_, f)\n",
    "        f.close()-\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results for each dataset\n",
    "\n",
    "#### for original data\n",
    "Executing model -  lr\n",
    "Best score on training set:  0.470703964588\n",
    "Accuracy:  0.99877\n",
    "Cohen Kappa: 0.57675\n",
    "Recall: 0.45652\n",
    "\n",
    "\n",
    "Executing model -  rfc\n",
    "Best score on training set:  0.594077266101\n",
    "Accuracy:  0.99916\n",
    "Cohen Kappa: 0.72145\n",
    "Recall: 0.59239\n",
    "\n",
    "Executing model -  gbc\n",
    "Best score on training set:  0.743152877901\n",
    "Accuracy:  0.99906\n",
    "Cohen Kappa: 0.7181\n",
    "Recall: 0.65217\n",
    "\n",
    "\n",
    "#### for resampled data - SMOTEENN\n",
    "Executing model -  lr\n",
    "Accuracy:  0.98976\n",
    "Cohen Kappa: 0.23847\n",
    "Recall: 0.88043\n",
    "\n",
    "Executing model -  rfc\n",
    "Accuracy:  0.99964\n",
    "Cohen Kappa: 0.89006\n",
    "Recall: 0.89024\n",
    "\n",
    "Executing model -  gbc\n",
    "Accuracy:  0.99466\n",
    "Cohen Kappa: 0.35888\n",
    "Recall: 0.91463\n",
    "\n",
    "#### for resampled data - SMOTETOMEK\n",
    "Executing model -  lr\n",
    "Accuracy:  0.98948\n",
    "Cohen Kappa: 0.22473\n",
    "Recall: 0.93902\n",
    "\n",
    "Executing model -  rfc\n",
    "Accuracy:  0.99964\n",
    "Cohen Kappa: 0.88939\n",
    "Recall: 0.88415\n",
    "\n",
    "Executing model -  gbc\n",
    "Accuracy:  0.99365\n",
    "Cohen Kappa: 0.3197\n",
    "Recall: 0.91463\n",
    "\n",
    "### without new featureas\n",
    "Executing model -  lr\n",
    "Accuracy:  0.99059\n",
    "Cohen Kappa: 0.24503\n",
    "Recall: 0.93617\n",
    "\n",
    "Executing model -  rfc\n",
    "Accuracy:  0.99964\n",
    "Cohen Kappa: 0.8895\n",
    "Recall: 0.88652\n",
    "\n",
    "Executing model -  gbc\n",
    "Accuracy:  0.99393\n",
    "Cohen Kappa: 0.33024\n",
    "Recall: 0.91489\n",
    "\n",
    "### SMOTETomek method, with no additional features and without shuffling the data\n",
    "Executing model -  lr\n",
    "Accuracy:  0.98425\n",
    "Cohen Kappa: 0.15936\n",
    "Recall: 0.92199\n",
    "\n",
    "Executing model -  rfc\n",
    "Accuracy:  0.9996\n",
    "Cohen Kappa: 0.88008\n",
    "Recall: 0.88652\n",
    "\n",
    "Executing model -  gbc\n",
    "Accuracy:  0.99022\n",
    "Cohen Kappa: 0.23781\n",
    "Recall: 0.93617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see resampled data (SMOTETomek method) performs best with a recall of 94% for simple logistic regression and gradient boosted trees method<br>\n",
    "\n",
    "### Evaluation in imbalanced domains\n",
    "Traditionally, the accuracy rate has been the most commonly used empirical measure.\n",
    "However in case of imbalanced data, it cannot distinguish between the number of correctly classified examples of different classes. Comparing some of the metrics below : \n",
    "1. The <b> Area Under the ROC curve (AUC)</b> is equal to the probability that a random positive example will be ranked above a random negative example.\n",
    "2. The <b> F1 Score</b> is the harmonic mean of precision and recall. \n",
    "3. <b> Cohenâ€™s Kappa</b> is an evaluation statistic that takes into account how much agreement would be expected by chance\n",
    "4. <b> Recall/Sensitivity </b> is the ability of a test to correctly identify true positive rate.\n",
    "5. <b> Precision Recall curve </b> The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n",
    "I have used <b>Recall</b> as the metric to evaluate the model as it  helps identify how well the model performs for the more important but less frequent class(fraudulent transactions). PR curve can also be used, however, it is ineffective when the data is re-sampled which is the case for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward chaining cross validation technique for Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time-series is problematic for cross-validation bacause if a pattern emerges in year 3 and stays for year 5, then the model should be able to pick up on it, even though it wasn't part of years 1 & 2. <br>\n",
    "For this we can use the forward chaining technique: <br>\n",
    "fold 1 : training [1], test [2] <br>\n",
    "fold 2 : training [1 2], test [3] <br>\n",
    "fold 3 : training [1 2 3], test [4] <br>\n",
    "fold 4 : training [1 2 3 4], test [5] <br>\n",
    "<br>\n",
    "This is a possible approach for this problem if we had some continuance in the data or if we were modelling transactions over a period of time. However, the question doesn't explicitly state the exact time of transactions(it is relative) and hence this technique will not be explored further.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
